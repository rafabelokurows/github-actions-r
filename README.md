# Automated scraping with Github Actions and R

I had mainly two goals with this project:  

1. Learn how to use Git Hub Actions - at least in a very basic sense - to start automating boring data collection tasks that I do in projects at work or for fun.
2. Start collecting data that could prove useful in the future for various reasons.

Therefore, I've created three diferent R scripts to scrape and process data from different sources with varying degrees of difficulty and for different uses.

1 - Scraping US fuel prices from fuelconomy.gov - runs daily  

[![Scraping US gas prices](https://github.com/rafabelokurows/scrapingActions/actions/workflows/main.yml/badge.svg)](https://github.com/rafabelokurows/scrapingActions/actions/workflows/main.yml)

2 - Scraping Google traffic data with the Google Maps API - runs daily at 09 AM, Noon and 04 PM  

[![Obtaining traffic data](https://github.com/rafabelokurows/scrapingActions/actions/workflows/main2.yml/badge.svg)](https://github.com/rafabelokurows/scrapingActions/actions/workflows/main2.yml)

3 - Scraping store locations from key groceries chains in Portugal

[![Scraping Portuguese retailers locations](https://github.com/rafabelokurows/scrapingActions/actions/workflows/main3.yml/badge.svg)](https://github.com/rafabelokurows/scrapingActions/actions/workflows/main3.yml)

For now, I'm covering:

* Continente
* Pingo Doce
* Aldi
* Intermarché
* Minipreço (Dia %)

TO DO:  

* Lidl
* Auchan
